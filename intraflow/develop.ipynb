{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script develop.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '../sample_data/crop1_HCAEC_Pg33277_4h.tif'  # Replace with the actual path to your TIFF file\n",
    "# image_stack = tiff.imread(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fourier_transform(image):\n",
    "    f_transform = np.fft.fft2(image)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    return f_shift\n",
    "\n",
    "def apply_inverse_fourier_transform(f_shift):\n",
    "    f_ishift = np.fft.ifftshift(f_shift)\n",
    "    img_back = np.fft.ifft2(f_ishift)\n",
    "    img_back = np.abs(img_back)\n",
    "    return img_back\n",
    "\n",
    "def create_notch_filter(shape, centers, radius):\n",
    "    rows, cols = shape\n",
    "    mask = np.ones((rows, cols), np.uint8)\n",
    "    \n",
    "    x, y = np.ogrid[:rows, :cols]\n",
    "    for center in centers:\n",
    "        crow, ccol = center\n",
    "        mask_area = (x - crow)**2 + (y - ccol)**2 <= radius**2\n",
    "        mask[mask_area] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def process_image(image, notch_centers, notch_radius):\n",
    "    f_shift = apply_fourier_transform(image)\n",
    "    \n",
    "    notch_filter = create_notch_filter(image.shape, notch_centers, notch_radius)\n",
    "    \n",
    "    filtered_f_shift = f_shift * notch_filter\n",
    "    \n",
    "    filtered_image = apply_inverse_fourier_transform(filtered_f_shift)\n",
    "    \n",
    "    return filtered_image\n",
    "\n",
    "def enhance_image(image, method='scaling', clip_limit=2.0, tile_grid_size=(8, 8), alpha=1.5, beta=50):\n",
    "    if method == 'histogram_equalization':\n",
    "        enhanced_image = cv2.equalizeHist(image.astype(np.uint8))\n",
    "    elif method == 'clahe':\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "        enhanced_image = clahe.apply(image.astype(np.uint8))\n",
    "    elif method == 'scaling':\n",
    "        enhanced_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid enhancement method. Choose 'histogram_equalization', 'clahe', or 'scaling'.\")\n",
    "    return enhanced_image\n",
    "\n",
    "def normalize_intensity_across_stack(image_stack):\n",
    "    # Calculate global mean and std deviation for the entire stack\n",
    "    all_pixels = np.concatenate([image.flatten() for image in image_stack])\n",
    "    global_mean = np.mean(all_pixels)\n",
    "    global_std = np.std(all_pixels)\n",
    "    \n",
    "    # Normalize each image in the stack\n",
    "    normalized_stack = []\n",
    "    for image in image_stack:\n",
    "        image = (image - np.mean(image)) / np.std(image)  # Standardize\n",
    "        image = image * global_std + global_mean  # Scale to global stats\n",
    "        image = np.clip(image, 0, 255)  # Clip to valid intensity range\n",
    "        normalized_stack.append(image.astype(np.uint8))\n",
    "    \n",
    "    return np.array(normalized_stack)\n",
    "\n",
    "def adjust_brightness_contrast(image, brightness=1.0, contrast=1.0):\n",
    "    # For user-defined visualization\n",
    "    image=image*brightness\n",
    "    mean_img = np.mean(image)\n",
    "    image = (image-mean_img)*contrast+mean_img\n",
    "    image = np.clip(image,0,255)\n",
    "    return image.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_tiff(input_path, enhancement_method, alpha, beta=10, output_path=None, colormap=\"hot\"):\n",
    "    # Load the image stack\n",
    "    image_stack = tiff.imread(input_path)\n",
    "\n",
    "    # Define the centers and radius for the notch filters (based on the interference pattern)\n",
    "    notch_centers = [(image_stack[0].shape[0]//2, image_stack[0].shape[1]//2)]  # Adjust this based on the pattern\n",
    "    notch_radius = 20  # Adjust this based on the pattern\n",
    "\n",
    "    # Process and enhance each image in the stack\n",
    "    processed_stack = []\n",
    "    for i, original_image in enumerate(image_stack):\n",
    "        filtered_image = process_image(original_image, notch_centers, notch_radius)\n",
    "        filtered_image = cv2.normalize(filtered_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        enhanced_image = enhance_image(filtered_image, method=enhancement_method, alpha=alpha, beta=beta)\n",
    "        processed_stack.append(enhanced_image)\n",
    "\n",
    "    # Normalize intensity across the entire stack\n",
    "    normalized_stack = normalize_intensity_across_stack(processed_stack)\n",
    "\n",
    "    # Save the normalized stack to a new TIFF file\n",
    "    if output_path:\n",
    "        tiff.imwrite(output_path, normalized_stack)\n",
    "        print(\"Your images are enhanced and saved at {}\".format(output_path))\n",
    "\n",
    "    # Optionally, display the images\n",
    "    fig, axes=plt.subplots(1,3,figsize=(15,5))\n",
    "    axes[0].imshow(image_stack[0], cmap=colormap)\n",
    "    axes[0].set_title('First Raw Image')\n",
    "    \n",
    "    adjusted_image=adjust_brightness_contrast(image_stack[0], brightness=1.0, contrast=1.0)\n",
    "    axes[1].imshow(adjusted_image, cmap=colormap)\n",
    "    axes[1].set_title('First Adjusted Image')\n",
    "\n",
    "    axes[2].imshow(normalized_stack[0], cmap=colormap)\n",
    "    axes[2].set_title('First Normalized Enhanced Image')\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# input_file_path = '../sample_data/crop1_HCAEC_Pg33277_4h.tif'\n",
    "# output_file_path = '../../figs/normalized_image_stack.tif'\n",
    "# enhancement_method = 'scaling'\n",
    "# alpha = 10\n",
    "# beta = 10\n",
    "\n",
    "# # test\n",
    "# process_and_save_tiff(input_file_path, enhancement_method, alpha, beta, output_file_path, colormap='hot')\n",
    "# # test_passed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store the selected center\n",
    "selected_center = None\n",
    "\n",
    "def select_center(event, x, y, flags, param):\n",
    "    global selected_center\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        selected_center = (x, y)\n",
    "        print(f\"Selected center: {selected_center}\")\n",
    "\n",
    "def load_image_stack(file_path):\n",
    "    \"\"\"Load the image stack from a TIFF file.\"\"\"\n",
    "    return tiff.imread(file_path)\n",
    "\n",
    "def display_initial_image(image):\n",
    "    \"\"\"Display the initial image and allow the user to select the center.\"\"\"\n",
    "    cv2.namedWindow(\"Select Center\")\n",
    "    cv2.setMouseCallback(\"Select Center\", select_center)\n",
    "    \n",
    "    while True:\n",
    "        cv2.imshow(\"Select Center\", image)\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or selected_center is not None:  # ESC key to exit\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    return selected_center\n",
    "\n",
    "def threshold_image(image, threshold_value=128):\n",
    "    \"\"\"Threshold the image to create a binary image.\"\"\"\n",
    "    _, binary_image = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    return binary_image\n",
    "\n",
    "def extract_coordinates(binary_image):\n",
    "    \"\"\"Extract coordinates of bright spots from the binary image.\"\"\"\n",
    "    coordinates = np.column_stack(np.where(binary_image > 0))\n",
    "    return coordinates\n",
    "\n",
    "def apply_dbscan(coordinates, eps=2, min_samples=2):\n",
    "    \"\"\"Apply DBSCAN clustering to the coordinates.\"\"\"\n",
    "    if len(coordinates) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(coordinates)\n",
    "    labels = db.labels_\n",
    "    # Filter out noise points\n",
    "    core_coords = coordinates[labels != -1]\n",
    "    core_labels = labels[labels != -1]\n",
    "    return core_coords, core_labels\n",
    "\n",
    "def draw_clusters(image, coordinates, labels, line_thickness=5):\n",
    "    \"\"\"Draw clusters on the original image with inverted colors and thicker lines.\"\"\"\n",
    "    inverted_image = cv2.bitwise_not(image)  # Invert the image colors\n",
    "    output_image = cv2.cvtColor(inverted_image, cv2.COLOR_GRAY2RGB)\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            continue\n",
    "        class_member_mask = (labels == label)\n",
    "        xy = coordinates[class_member_mask]\n",
    "        for point in xy:\n",
    "            cv2.circle(output_image, (point[1], point[0]), line_thickness, (0, 0, 255), -1)  # Red points\n",
    "    return output_image\n",
    "\n",
    "def process_image_stack(file_path, eps=2, min_samples=2, threshold_value=128, output_path=None, line_thickness=5):\n",
    "    \"\"\"Process the entire image stack with DBSCAN and save the results.\"\"\"\n",
    "    # Load the image stack\n",
    "    image_stack = load_image_stack(file_path)\n",
    "    \n",
    "    # Store DBSCAN coordinates for later use\n",
    "    dbscan_coords_stack = []\n",
    "    \n",
    "    # Process each image in the stack\n",
    "    result_stack = []\n",
    "    for i, image in enumerate(image_stack):\n",
    "        # Threshold the image to get a binary image\n",
    "        binary_image = threshold_image(image, threshold_value)\n",
    "        \n",
    "        # Extract coordinates of potential vesicle positions\n",
    "        coordinates = extract_coordinates(binary_image)\n",
    "        \n",
    "        # Apply DBSCAN to cluster the coordinates\n",
    "        core_coords, core_labels = apply_dbscan(coordinates, eps, min_samples)\n",
    "        \n",
    "        # Store the DBSCAN-processed coordinates\n",
    "        dbscan_coords_stack.append(core_coords)\n",
    "        \n",
    "        # Draw clusters on the original image\n",
    "        result_image = draw_clusters(image, core_coords, core_labels, line_thickness)\n",
    "        \n",
    "        # Add the result image to the result stack\n",
    "        result_stack.append(result_image)\n",
    "    \n",
    "    # Convert the result stack to a NumPy array\n",
    "    result_stack = np.array(result_stack)\n",
    "    \n",
    "    # Save the result stack to a new TIFF file\n",
    "    if output_path:\n",
    "        tiff.imwrite(output_path, result_stack)\n",
    "    \n",
    "    return dbscan_coords_stack\n",
    "\n",
    "def cartesian_to_polar(coordinates, center):\n",
    "    \"\"\"Convert Cartesian coordinates to polar coordinates.\"\"\"\n",
    "    x = coordinates[:, 1] - center[0]\n",
    "    y = coordinates[:, 0] - center[1]\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    polar_coordinates = np.column_stack((rho, phi))\n",
    "    return polar_coordinates\n",
    "\n",
    "def save_image_stack(file_path, image_stack):\n",
    "    \"\"\"Save the image stack to a TIFF file.\"\"\"\n",
    "    tiff.imwrite(file_path, image_stack)\n",
    "\n",
    "def convert_coords_stack_to_polar(polar_image_path,dbscan_coords_stack, center, max_r, line_thickness=5):\n",
    "    \"\"\"Convert the DBSCAN coordinates in the image stack to polar coordinates.\"\"\"\n",
    "    polar_stack = []\n",
    "    for coords in dbscan_coords_stack:\n",
    "        polar_image = np.ones((int(max_r), 360, 3), dtype=np.uint8) * 255  # Create a white image with dimensions (max_r, 360, 3)\n",
    "        if coords.size > 0:\n",
    "            polar_coordinates = cartesian_to_polar(coords, center)\n",
    "            rho = polar_coordinates[:, 0]\n",
    "            phi = np.degrees(polar_coordinates[:, 1]) + 180  # Convert radians to degrees and shift to 0-360\n",
    "            rho = np.clip(rho, 0, max_r - 1).astype(int)\n",
    "            phi = np.clip(phi, 0, 359).astype(int)\n",
    "            for r, p in zip(rho, phi):\n",
    "                cv2.circle(polar_image, (p, r), line_thickness, (255, 0, 0), -1)  # Draw thicker red points\n",
    "        polar_stack.append(polar_image)\n",
    "    # Save the polar-converted image stack\n",
    "    save_image_stack(polar_image_path, np.array(polar_stack))\n",
    "    print(f\"Polar converted image stack saved to {polar_image_path}\")\n",
    "    return np.array(polar_stack)\n",
    "\n",
    "\n",
    "def detect_vesicles_and_convert_to_polar(input_file_path, dbscan_output_file_path, polar_output_file_path, eps=2, min_samples=2, threshold_value=128, line_thickness=5):\n",
    "    # Ensure dbscan_output_file_path is a file path, not a directory\n",
    "    if os.path.isdir(dbscan_output_file_path):\n",
    "        dbscan_output_file_path = os.path.join(dbscan_output_file_path, \"dbscan_detected_vesicle_stack.tif\")\n",
    "    \n",
    "    # Ensure polar_output_file_path is a file path, not a directory\n",
    "    if os.path.isdir(polar_output_file_path):\n",
    "        polar_output_file_path = os.path.join(polar_output_file_path, \"polar_converted_stack.tif\")\n",
    "    \n",
    "    # Process the image stack with DBSCAN and save the results\n",
    "    dbscan_coords_stack = process_image_stack(\n",
    "        file_path=input_file_path, \n",
    "        eps=eps, \n",
    "        min_samples=min_samples, \n",
    "        threshold_value=threshold_value, \n",
    "        output_path=dbscan_output_file_path, \n",
    "        line_thickness=line_thickness\n",
    "    )\n",
    "    \n",
    "    # Load the initial image for center selection\n",
    "    initial_image = tiff.imread(input_file_path)[0]\n",
    "    center = display_initial_image(initial_image)\n",
    "    \n",
    "    # Calculate the maximum radius for polar conversion\n",
    "    max_r = np.sqrt(initial_image.shape[0]**2 + initial_image.shape[1]**2)\n",
    "    \n",
    "    # Convert coordinates to polar and save the polar stack\n",
    "    polar_stack = convert_coords_stack_to_polar(polar_output_file_path, dbscan_coords_stack, center, max_r, line_thickness)\n",
    "    \n",
    "    return\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage for DBSCAN processing\n",
    "# file_path = '../../figs/normalized_image_stack.tif'  # Replace with the actual path to your TIFF file\n",
    "# dbscan_output_path = '../../figs/dbscan_detected_vesicle_stack.tif'  # Replace with the desired output path for DBSCAN results\n",
    "# polar_output_path = '../../figs/polar_converted_stack.tif'  # Replace with the desired output path for polar conversion results\n",
    "# eps = 2  # Adjust based on expected vesicle size\n",
    "# min_samples = 2  # Adjust based on expected density of vesicle clusters\n",
    "# threshold_value = 128  # Adjust based on intensity threshold for vesicle detection\n",
    "# line_thickness = 5  # Thickness of the line used to label the detected vesicles\n",
    "# detect_vesicles_and_convert_to_polar(file_path, dbscan_output_path, polar_output_path, eps, min_samples, threshold_value, line_thickness)\n",
    "\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spot_locations(image, threshold_value=128):\n",
    "    \"\"\"Extract coordinates of spots from the binary image.\"\"\"\n",
    "    _, binary_image = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    coordinates = np.column_stack(np.where(binary_image > 0))\n",
    "    return coordinates\n",
    "\n",
    "def find_moving_spots_with_vectors(coords1, coords2, window_size=5):\n",
    "    \"\"\"Find spots in coords1 that have corresponding spots in coords2 within the window size and calculate their movement vectors.\"\"\"\n",
    "    moving_spots = []\n",
    "    vectors = []\n",
    "    for (y1, x1) in coords1[:, :2]:  # Ensure only y and x coordinates are used\n",
    "        # Limit the search to a window around (y1, x1)\n",
    "        window_coords = coords2[(abs(coords2[:, 0] - y1) <= window_size) & (abs(coords2[:, 1] - x1) <= window_size)][:, :2]\n",
    "        for (y2, x2) in window_coords:\n",
    "            if abs(y1 - y2) <= window_size and abs(x1 - x2) <= window_size:\n",
    "                moving_spots.append((y1, x1))\n",
    "                vectors.append((y2 - y1, x2 - x1))\n",
    "                break\n",
    "    return np.array(moving_spots), np.array(vectors)\n",
    "\n",
    "def calculate_angles_and_color(moving_spots, vectors, colormap='bwr'):\n",
    "    \"\"\"Calculate the angles between the movement vectors and the vectors from the origin and apply colormap.\"\"\"\n",
    "    origin = (180, 0)\n",
    "    colors = []\n",
    "    for i, (spot, vector) in enumerate(zip(moving_spots, vectors)):\n",
    "        vector1 = np.array([spot[0] - origin[0], spot[1] - origin[1]])\n",
    "        vector2 = vector\n",
    "        norm1 = np.linalg.norm(vector1)\n",
    "        norm2 = np.linalg.norm(vector2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            angle = 90  # Default to 90 degrees if either vector is zero to indicate no movement\n",
    "        else:\n",
    "            dot_product = np.dot(vector1, vector2)\n",
    "            # Ensure the dot product result is within the valid range for arccos\n",
    "            dot_product = np.clip(dot_product / (norm1 * norm2), -1.0, 1.0)\n",
    "            angle = np.degrees(np.arccos(dot_product))\n",
    "        \n",
    "        # Normalize the angle to the range [0, 180]\n",
    "        angle = min(angle, 180 - angle)\n",
    "        \n",
    "        # Determine the color based on the angle\n",
    "        if angle < 90:\n",
    "            color = angle / 90  # Scale to [0, 1]\n",
    "        else:\n",
    "            color = (angle - 90) / 90  # Scale to [0, 1]\n",
    "        \n",
    "        colors.append(color)\n",
    "    \n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "    cmap = cm.get_cmap(colormap)\n",
    "    colored_spots = [cmap(norm(color)) for color in colors]\n",
    "    \n",
    "    return colored_spots\n",
    "\n",
    "\n",
    "def process_and_save_overlay(polar_image_path, initial_range=(30, 40), window_size=5, colormap='bwr'):\n",
    "    # Load the polar-converted image stack\n",
    "    polar_stack = load_image_stack(polar_image_path)\n",
    "\n",
    "    # Create subplots\n",
    "    num_images = initial_range[1] - initial_range[0]\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 5))  # Adjust figsize for better spacing\n",
    "\n",
    "    # Process each pair of consecutive images in the specified range of the stack\n",
    "    for i, ax in zip(tqdm(range(initial_range[0], initial_range[1]), desc=\"Processing images\"), axes):\n",
    "        first_image = polar_stack[i]\n",
    "        second_image = polar_stack[i + 1]\n",
    "\n",
    "        # Invert the images to make the background white and spots black\n",
    "        first_image_inverted = cv2.bitwise_not(first_image)\n",
    "        second_image_inverted = cv2.bitwise_not(second_image)\n",
    "\n",
    "        # Extract spot locations from both images\n",
    "        coords1 = extract_spot_locations(first_image_inverted)\n",
    "        coords2 = extract_spot_locations(second_image_inverted)\n",
    "\n",
    "        # Find moving spots and their movement vectors\n",
    "        moving_spots, vectors = find_moving_spots_with_vectors(coords1, coords2, window_size)\n",
    "\n",
    "        # Calculate angles and determine colors\n",
    "        colored_spots = calculate_angles_and_color(moving_spots, vectors, colormap)\n",
    "\n",
    "        # Create an image to display\n",
    "        overlay_image = np.ones((polar_stack.shape[1], polar_stack.shape[2], 3), dtype=np.float32)  # Initialize with white background\n",
    "        for (spot, color) in zip(moving_spots, colored_spots):\n",
    "            cv2.circle(overlay_image, (spot[1], spot[0]), 10, color, -1)\n",
    "\n",
    "        # Display the overlay image in the subplot\n",
    "        ax.imshow(overlay_image)\n",
    "        ax.set_title(f'Vesicle Movement {i+1} to {i+2}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Adjust layout to reduce space between plots\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)  # Reduce space between plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# polar_image_path = '../../figs/polar_converted_stack.tif'\n",
    "# overlay_output_path = '../../figs/vesicle_movement_colored.tif'\n",
    "# process_and_save_overlay(polar_image_path, initial_range=(30, 32), window_size=5, colormap='bwr')\n",
    "# # test passed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intraflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
